{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aAmohammadrezaaA/Retinal-Vessel-Segmentation_A-Computer-Vision-Technique/blob/main/implementation_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "from prettytable import PrettyTable\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import AveragePooling2D,Conv2DTranspose,Input,Add,Conv2D, BatchNormalization,LeakyReLU, Activation, MaxPool2D, Dropout, Flatten, Dense,UpSampling2D,Concatenate,Softmax"
      ],
      "metadata": {
        "id": "d4E8HHv8sLCr"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_size=48\n",
        "patch_num=1500\n",
        "patch_threshold=25\n",
        "BATCH_SIZE=64\n",
        "LR=0.0003\n",
        "channels=3"
      ],
      "metadata": {
        "id": "FAZqlwTN1z9I"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearTransform(tf.keras.Model):\n",
        "  def __init__(self, name=\"LinearTransform\"):\n",
        "    super(LinearTransform, self).__init__(self,name=name)\n",
        "\n",
        "    self.conv_r=Conv2D(1,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "    self.conv_g=Conv2D(1,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "    self.conv_b=Conv2D(1,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "\n",
        "    self.pool_rc=AveragePooling2D(pool_size=(patch_size,patch_size),strides=1)\n",
        "    self.pool_gc=AveragePooling2D(pool_size=(patch_size,patch_size),strides=1)\n",
        "    self.pool_bc=AveragePooling2D(pool_size=(patch_size,patch_size),strides=1)\n",
        "\n",
        "    self.bn=BatchNormalization()\n",
        "    self.sigmoid=Activation('sigmoid')\n",
        "    self.softmax=Activation('softmax')\n",
        "\n",
        "  def call(self, input,training=True):\n",
        "    r,g,b=input[:,:,:,0:1],input[:,:,:,1:2],input[:,:,:,2:3]\n",
        "\n",
        "    rs=self.conv_r(r)\n",
        "    gs=self.conv_g(g)\n",
        "    bs=self.conv_r(b)\n",
        "\n",
        "    rc=tf.reshape(self.pool_rc(rs),[-1,1])\n",
        "    gc=tf.reshape(self.pool_gc(gs),[-1,1])\n",
        "    bc=tf.reshape(self.pool_bc(bs),[-1,1])\n",
        "\n",
        "    merge=Concatenate(axis=-1)([rc,gc,bc])\n",
        "    merge=tf.expand_dims(merge,axis=1)\n",
        "    merge=tf.expand_dims(merge,axis=1)\n",
        "    merge=self.softmax(merge)\n",
        "    merge=tf.repeat(merge,repeats=48,axis=2)\n",
        "    merge=tf.repeat(merge,repeats=48,axis=1)\n",
        "\n",
        "    r=r*(1+self.sigmoid(rs))\n",
        "    g=g*(1+self.sigmoid(gs))\n",
        "    b=b*(1+self.sigmoid(bs))\n",
        "\n",
        "    output=self.bn(merge[:,:,:,0:1]*r+merge[:,:,:,1:2]*g+merge[:,:,:,2:3]*b,training=training)\n",
        "    return output\n",
        "\n",
        "class ResBlock(tf.keras.Model):\n",
        "  def __init__(self,out_ch,residual_path=False,stride=1):\n",
        "    super(ResBlock,self).__init__(self)\n",
        "    self.residual_path=residual_path\n",
        "\n",
        "    self.conv1=Conv2D(out_ch,kernel_size=3,strides=stride,padding='same', use_bias=False,data_format=\"channels_last\")\n",
        "    self.bn1=BatchNormalization()\n",
        "    self.relu1=LeakyReLU()#Activation('leaky_relu')\n",
        "\n",
        "    self.conv2=Conv2D(out_ch,kernel_size=3,strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
        "    self.bn2=BatchNormalization()\n",
        "\n",
        "    if residual_path:\n",
        "      self.conv_shortcut=Conv2D(out_ch,kernel_size=1,strides=stride,padding='same',use_bias=False)\n",
        "      self.bn_shortcut=BatchNormalization()\n",
        "\n",
        "    self.relu2=LeakyReLU()#Activation('leaky_relu')\n",
        "\n",
        "  def call(self,x,training=True):\n",
        "    xs=self.relu1(self.bn1(self.conv1(x),training=training))\n",
        "    xs=self.bn2(self.conv2(xs),training=training)\n",
        "\n",
        "    if self.residual_path:\n",
        "      x=self.bn_shortcut(self.conv_shortcut(x),training=training)\n",
        "    #print(x.shape,xs.shape)\n",
        "    xs=x+xs\n",
        "    return self.relu2(xs)\n",
        "\n",
        "class Unet(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Unet,self).__init__(self)\n",
        "    self.conv_init=LinearTransform()\n",
        "    self.resinit=ResBlock(16,residual_path=True)\n",
        "    self.up_sample=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
        "    self.resup=ResBlock(32,residual_path=True)\n",
        "\n",
        "    self.pool1=MaxPool2D(pool_size=(2,2))\n",
        "\n",
        "    self.resblock_down1=ResBlock(64,residual_path=True)\n",
        "    self.resblock_down11=ResBlock(64,residual_path=False)\n",
        "    self.pool2=MaxPool2D(pool_size=(2,2))\n",
        "\n",
        "    self.resblock_down2=ResBlock(128,residual_path=True)\n",
        "    self.resblock_down21=ResBlock(128,residual_path=False)\n",
        "    self.pool3=MaxPool2D(pool_size=(2,2))\n",
        "\n",
        "    self.resblock_down3=ResBlock(256,residual_path=True)\n",
        "    self.resblock_down31=ResBlock(256,residual_path=False)\n",
        "    self.pool4=MaxPool2D(pool_size=(2,2))\n",
        "\n",
        "    self.resblock=ResBlock(512,residual_path=True)\n",
        "\n",
        "    self.unpool3=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
        "    self.resblock_up3=ResBlock(256,residual_path=True)\n",
        "    self.resblock_up31=ResBlock(256,residual_path=False)\n",
        "\n",
        "    self.unpool2=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
        "    self.resblock_up2=ResBlock(128,residual_path=True)\n",
        "    self.resblock_up21=ResBlock(128,residual_path=False)\n",
        "\n",
        "    self.unpool1=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
        "    self.resblock_up1=ResBlock(64,residual_path=True)\n",
        "\n",
        "    self.unpool_final=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
        "    self.resblock2=ResBlock(32,residual_path=True)\n",
        "\n",
        "    self.pool_final=MaxPool2D(pool_size=(2,2))\n",
        "    self.resfinal=ResBlock(32)\n",
        "\n",
        "    self.conv_final=Conv2D(1,kernel_size=1,strides=1,padding='same',use_bias=False)\n",
        "    self.bn_final=BatchNormalization()\n",
        "    self.act=Activation('sigmoid')\n",
        "\n",
        "  def call(self,x,training=True):\n",
        "    x_linear=self.conv_init(x,training=training)\n",
        "    x=self.resinit(x_linear,training=training)\n",
        "    x=self.up_sample(x)\n",
        "    x=self.resup(x,training=training)\n",
        "\n",
        "    stage1=self.pool1(x)\n",
        "    stage1=self.resblock_down1(stage1,training=training)\n",
        "    stage1=self.resblock_down11(stage1,training=training)\n",
        "\n",
        "    stage2=self.pool2(stage1)\n",
        "    stage2=self.resblock_down2(stage2,training=training)\n",
        "    stage2=self.resblock_down21(stage2,training=training)\n",
        "\n",
        "    stage3=self.pool3(stage2)\n",
        "    stage3=self.resblock_down3(stage3,training=training)\n",
        "    stage3=self.resblock_down31(stage3,training=training)\n",
        "\n",
        "    stage4=self.pool4(stage3)\n",
        "    stage4=self.resblock(stage4,training=training)\n",
        "\n",
        "    stage3=Concatenate(axis=3)([stage3,self.unpool3(stage4)])\n",
        "    stage3=self.resblock_up3(stage3,training=training)\n",
        "    stage3=self.resblock_up31(stage3,training=training)\n",
        "\n",
        "    stage2=Concatenate(axis=3)([stage2,self.unpool2(stage3)])\n",
        "    stage2=self.resblock_up2(stage2,training=training)\n",
        "    stage2=self.resblock_up21(stage2,training=training)\n",
        "\n",
        "    stage1=Concatenate(axis=3)([stage1,self.unpool1(stage2)])\n",
        "    stage1=self.resblock_up1(stage1,training=training)\n",
        "\n",
        "    x=Concatenate(axis=3)([x,self.unpool_final(stage1)])\n",
        "    x=self.resblock2(x,training=training)\n",
        "\n",
        "    x=self.pool_final(x)\n",
        "    x=self.resfinal(x,training=training)\n",
        "\n",
        "    seg_result=self.act(self.bn_final(self.conv_final(x),training=training))\n",
        "\n",
        "    return x_linear,seg_result\n",
        "class Unet2(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Unet2,self).__init__(self)\n",
        "    self.conv_init=LinearTransform()\n",
        "    #self.resinit=ResBlock(16,residual_path=True)\n",
        "    self.resinit=Conv2D(16,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "    self.up_sample=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
        "    #self.resup=ResBlock(32,residual_path=True)\n",
        "    self.resup=Conv2D(32,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "\n",
        "    self.pool1=MaxPool2D(pool_size=(2,2))\n",
        "\n",
        "    #self.resblock_down1=ResBlock(64,residual_path=True)\n",
        "    self.resblock_down1=Conv2D(64,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "    #self.resblock_down11=ResBlock(64,residual_path=False)\n",
        "    self.resblock_down11=Conv2D(64,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "    self.pool2=MaxPool2D(pool_size=(2,2))\n",
        "\n",
        "    #self.resblock_down2=ResBlock(128,residual_path=True)\n",
        "    self.resblock_down2=Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "    #self.resblock_down21=ResBlock(128,residual_path=False)\n",
        "    self.resblock_down21=Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "    self.pool3=MaxPool2D(pool_size=(2,2))\n",
        "\n",
        "    #self.resblock_down3=ResBlock(256,residual_path=True)\n",
        "    self.resblock_down3=Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "    #self.resblock_down31=ResBlock(256,residual_path=False)\n",
        "    self.resblock_down31=Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "    self.pool4=MaxPool2D(pool_size=(2,2))\n",
        "\n",
        "    #self.resblock=ResBlock(512,residual_path=True)\n",
        "    self.resblock=Conv2D(512,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "\n",
        "    self.unpool3=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
        "    #self.resblock_up3=ResBlock(256,residual_path=True)\n",
        "    self.resblock_up3=Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "    #self.resblock_up31=ResBlock(256,residual_path=False)\n",
        "    self.resblock_up31=Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "\n",
        "    self.unpool2=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
        "    #self.resblock_up2=ResBlock(128,residual_path=True)\n",
        "    self.resblock_up2=Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "    #self.resblock_up21=ResBlock(128,residual_path=False)\n",
        "    self.resblock_up21=Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "\n",
        "    self.unpool1=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
        "    #self.resblock_up1=ResBlock(64,residual_path=True)\n",
        "    self.resblock_up1=Conv2D(64,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "\n",
        "    self.unpool_final=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
        "    #self.resblock2=ResBlock(32,residual_path=True)\n",
        "    self.resblock2=Conv2D(32,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "\n",
        "    self.pool_final=MaxPool2D(pool_size=(2,2))\n",
        "    #self.resfinal=ResBlock(32)\n",
        "    self.resfinal=Conv2D(32,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
        "\n",
        "    self.conv_final=Conv2D(1,kernel_size=1,strides=1,padding='same',use_bias=False)\n",
        "    self.bn_final=BatchNormalization()\n",
        "    self.act=Activation('sigmoid')\n",
        "\n",
        "  def call(self,x,training=True):\n",
        "    x_linear=self.conv_init(x,training=training)\n",
        "    x=self.resinit(x_linear,training=training)\n",
        "    x=self.up_sample(x)\n",
        "    x=self.resup(x,training=training)\n",
        "\n",
        "    stage1=self.pool1(x)\n",
        "    stage1=self.resblock_down1(stage1,training=training)\n",
        "    stage1=self.resblock_down11(stage1,training=training)\n",
        "\n",
        "    stage2=self.pool2(stage1)\n",
        "    stage2=self.resblock_down2(stage2,training=training)\n",
        "    stage2=self.resblock_down21(stage2,training=training)\n",
        "\n",
        "    stage3=self.pool3(stage2)\n",
        "    stage3=self.resblock_down3(stage3,training=training)\n",
        "    stage3=self.resblock_down31(stage3,training=training)\n",
        "\n",
        "    stage4=self.pool4(stage3)\n",
        "    stage4=self.resblock(stage4,training=training)\n",
        "\n",
        "    stage3=Concatenate(axis=3)([stage3,self.unpool3(stage4)])\n",
        "    stage3=self.resblock_up3(stage3,training=training)\n",
        "    stage3=self.resblock_up31(stage3,training=training)\n",
        "\n",
        "    stage2=Concatenate(axis=3)([stage2,self.unpool2(stage3)])\n",
        "    stage2=self.resblock_up2(stage2,training=training)\n",
        "    stage2=self.resblock_up21(stage2,training=training)\n",
        "\n",
        "    stage1=Concatenate(axis=3)([stage1,self.unpool1(stage2)])\n",
        "    stage1=self.resblock_up1(stage1,training=training)\n",
        "\n",
        "    x=Concatenate(axis=3)([x,self.unpool_final(stage1)])\n",
        "    x=self.resblock2(x,training=training)\n",
        "\n",
        "    x=self.pool_final(x)\n",
        "    x=self.resfinal(x,training=training)\n",
        "\n",
        "    seg_result=self.act(self.bn_final(self.conv_final(x),training=training))\n",
        "\n",
        "    return x_linear,seg_result"
      ],
      "metadata": {
        "id": "gLm3ZDh-52ue"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"#\", \"patch_size\", \"patch_num\", \"patch_threshold\", \"batch_size\", \"learning_rate\"]\n",
        "rows = [[ \"value\", patch_size, patch_num, patch_threshold, BATCH_SIZE, LR],\n",
        "        [\"comment\", \"(48*48) windows\", \"number of windows\", \"threshold for the patch, the smaller threshoold, the less vessel in the patch\", \"batch\", \"LR\"]]\n",
        "\n",
        "print(tabulate(rows, headers=cols,tablefmt=\"fancy_grid\"))"
      ],
      "metadata": {
        "id": "oj-hkaz0sK_t",
        "outputId": "ab10a1ff-eea2-40e1-c907-f87b5cef43e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒═════════╤═════════════════╤═══════════════════╤═══════════════════════════════════════════════════════════════════════════════╤══════════════╤═════════════════╕\n",
            "│ #       │ patch_size      │ patch_num         │ patch_threshold                                                               │ batch_size   │ learning_rate   │\n",
            "╞═════════╪═════════════════╪═══════════════════╪═══════════════════════════════════════════════════════════════════════════════╪══════════════╪═════════════════╡\n",
            "│ value   │ 48              │ 1500              │ 25                                                                            │ 64           │ 0.0003          │\n",
            "├─────────┼─────────────────┼───────────────────┼───────────────────────────────────────────────────────────────────────────────┼──────────────┼─────────────────┤\n",
            "│ comment │ (48*48) windows │ number of windows │ threshold for the patch, the smaller threshoold, the less vessel in the patch │ batch        │ LR              │\n",
            "╘═════════╧═════════════════╧═══════════════════╧═══════════════════════════════════════════════════════════════════════════════╧══════════════╧═════════════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (patch_size, patch_size, channels)  # Adjusting the shape\n",
        "batch_size = BATCH_SIZE\n",
        "\n",
        "# random input tensor for testing\n",
        "input_tensor = tf.random.normal((batch_size,) + input_shape)\n",
        "\n",
        "model_unet1=Unet()\n",
        "model_unet2=Unet2()\n",
        "linear_output1, seg_result1 = model_unet1(input_tensor)\n",
        "linear_output2, seg_result2 = model_unet2(input_tensor)\n",
        "#model.summary(line_length=110)\n",
        "#model_unet1.count_params()\n",
        "#model_unet2.count_params()"
      ],
      "metadata": {
        "id": "Ne9nQmN056ou"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_bc=0.0417;train_acc_bc=0.9228;train_f1_bc=0.5987;train_sp_bc=0.8952;train_se_bc=0.8952;train_precision_bc=0.894;train_auroc_bc=0.8952\n",
        "val_loss_bc=0.2181;val_acc_bc=0.8011;val_f1_bc=0.4935;val_sp_bc=0.8060;val_se_bc=0.8060;val_precision_bc=0.8054;val_auroc_bc=0.8060\n",
        "trained_till_epoch_bc=40; lowest_val_loss_on_epoch_bc=14\n",
        "\n",
        "train_loss_d=0.0782; train_acc_d=0.9218; train_f1_d=0.5979; train_sp_d=0.8926; train_se_d=0.8926; train_precision_d=0.8920; train_auroc_d=0.8926\n",
        "val_loss_d=0.1992; val_acc_d=0.8008; val_f1_d=0.4933; val_sp_d=0.8016; val_se_d=0.8016; val_precision_d=0.8011; val_auroc_d=0.8016\n",
        "trained_till_epoch_d=\"Nan\"; lowest_val_loss_on_epoch_d=\"Nan\"\n",
        "\n",
        "train_loss_f=170.6789; train_acc_f=0.8527; train_f1_f=0.5390; train_sp_f=0.7866; train_se_f=0.7040; train_precision_f=0.7859; train_auroc_f=0.7866\n",
        "val_loss_f=4232.2256; val_acc_f=0.7367; val_f1_f=0.4396; val_sp_f=0.7040; val_se_f=0.8016; val_precision_f=0.7028; val_auroc_f=0.7040\n",
        "trained_till_epoch_f=\"Nan\"; lowest_val_loss_on_epoch_f=\"Nan\"\n",
        "\n",
        "train_loss_u2_bc=0.0693; train_acc_u2_bc=0.8881; train_f1_u2_bc=0.5682; train_sp_u2_bc=0.8840; train_se_u2_bc=0.8840; train_precision_u2_bc=0.8833; train_auroc_u2_bc=0.8840\n",
        "val_loss_u2_bc=0.2361; val_acc_u2_bc=0.7802; val_f1_u2_bc=0.4762; val_sp_u2_bc=0.8205; val_se_u2_bc=0.8205; val_precision_u2_bc=0.8201; val_auroc_u2_bc=0.8205\n",
        "trained_till_epoch_u2_bc=71; lowest_val_loss_on_epoch_u2_bc=33;\n",
        "\n",
        "x = PrettyTable()\n",
        "x.field_names = [f\"Config of Net\", \"Net\", \"Number of parameters\", \"comments\"]\n",
        "x.add_row([\"report\", \"UNet1\", model_unet1.count_params(), \"The Unet architecture which uses residual blocks and residual path and skip connections\"])\n",
        "x.add_row([\"report\", \"UNet2\", model_unet2.count_params(), \"The same UNet, but replacing all the res blocks with Conv2D\"])\n",
        "print(x)\n",
        "x = PrettyTable()\n",
        "x.field_names = [f\"Metrics on train data\", \"Net\", \"loss_function\", \"trained_till_epoch\", f\"lowest val_loss which was on epoch\", \"train_loss\", \"train_acc\", \"train_f1\", \"train_specificity\", \"train_sensitivity\", \"train_precision\", \"train_auroc\"]\n",
        "x.add_row([\"report\", \"UNet1\", \"binary_crossentropy\", trained_till_epoch_bc, lowest_val_loss_on_epoch_bc, train_loss_bc, train_acc_bc, train_f1_bc, train_sp_bc, train_se_bc, train_precision_bc, train_auroc_bc])\n",
        "x.add_row([\"report\", \"UNet1\", \"dice_loss\", trained_till_epoch_d, lowest_val_loss_on_epoch_d, train_loss_d, train_acc_d, train_f1_d, train_sp_d, train_se_d, train_precision_d, train_auroc_d])\n",
        "x.add_row([\"report\", \"UNet1\", \"focal_loss\", trained_till_epoch_f, lowest_val_loss_on_epoch_f, train_loss_f, train_acc_f, train_f1_f, train_sp_f, train_se_f, train_precision_f, train_auroc_f])\n",
        "x.add_row([\"report\", \"UNet2\", \"binary_crossentropy\", trained_till_epoch_u2_bc, lowest_val_loss_on_epoch_u2_bc, train_loss_u2_bc, train_acc_u2_bc, train_f1_u2_bc, train_sp_u2_bc, train_se_u2_bc, train_precision_u2_bc, train_auroc_u2_bc])\n",
        "print(x)\n",
        "\n",
        "\n",
        "x = PrettyTable()\n",
        "x.field_names = [f\"Metrics on validation data\", \"Net\", \"loss_function\", \"trained_till_epoch\", \"lowest val_loss which was on epoch\", \"val_loss\", \"val_acc\", \"val_f1\", \"val_specificity\", \"val_sensitivity\", \"val_precision\", \"val_auroc\"]\n",
        "x.add_row([\"report\", \"UNet1\", \"binary_crossentropy\", trained_till_epoch_bc, lowest_val_loss_on_epoch_bc, val_loss_bc, val_acc_bc, val_f1_bc, val_sp_bc, val_se_bc, val_precision_bc, val_auroc_bc])\n",
        "x.add_row([\"report\", \"UNet1\", \"dice_loss\", trained_till_epoch_d, lowest_val_loss_on_epoch_d, val_loss_d, val_acc_d, val_f1_d, val_sp_d, val_se_d, val_precision_d, val_auroc_d])\n",
        "x.add_row([\"report\", \"UNet1\", \"focal_loss\", trained_till_epoch_f, lowest_val_loss_on_epoch_f, val_loss_f, val_acc_f, val_f1_f, val_sp_f, val_se_f, val_precision_f, val_auroc_f])\n",
        "x.add_row([\"report\", \"UNet2\", \"binary_crossentropy\", trained_till_epoch_u2_bc, lowest_val_loss_on_epoch_u2_bc, val_loss_u2_bc, val_acc_u2_bc, val_f1_u2_bc, val_sp_u2_bc, val_se_u2_bc, val_precision_u2_bc, val_auroc_u2_bc])\n",
        "print(x)\n"
      ],
      "metadata": {
        "id": "FOUS51M47Wtv",
        "outputId": "f1065238-617a-4b73-c3bb-3be7b80d7f4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------+----------------------+-----------------------------------------------------------------------------------------+\n",
            "| Config of Net |  Net  | Number of parameters |                                         comments                                        |\n",
            "+---------------+-------+----------------------+-----------------------------------------------------------------------------------------+\n",
            "|     report    | UNet1 |       11351706       | The Unet architecture which uses residual blocks and residual path and skip connections |\n",
            "|     report    | UNet2 |       5442250        |               The same UNet, but replacing all the res blocks with Conv2D               |\n",
            "+---------------+-------+----------------------+-----------------------------------------------------------------------------------------+\n",
            "+-----------------------+-------+---------------------+--------------------+------------------------------------+------------+-----------+----------+-------------------+-------------------+-----------------+-------------+\n",
            "| Metrics on train data |  Net  |    loss_function    | trained_till_epoch | lowest val_loss which was on epoch | train_loss | train_acc | train_f1 | train_specificity | train_sensitivity | train_precision | train_auroc |\n",
            "+-----------------------+-------+---------------------+--------------------+------------------------------------+------------+-----------+----------+-------------------+-------------------+-----------------+-------------+\n",
            "|         report        | UNet1 | binary_crossentropy |         40         |                 14                 |   0.0417   |   0.9228  |  0.5987  |       0.8952      |       0.8952      |      0.894      |    0.8952   |\n",
            "|         report        | UNet1 |      dice_loss      |        Nan         |                Nan                 |   0.0782   |   0.9218  |  0.5979  |       0.8926      |       0.8926      |      0.892      |    0.8926   |\n",
            "|         report        | UNet1 |      focal_loss     |        Nan         |                Nan                 |  170.6789  |   0.8527  |  0.539   |       0.7866      |       0.704       |      0.7859     |    0.7866   |\n",
            "|         report        | UNet2 | binary_crossentropy |         71         |                 33                 |   0.0693   |   0.8881  |  0.5682  |       0.884       |       0.884       |      0.8833     |    0.884    |\n",
            "+-----------------------+-------+---------------------+--------------------+------------------------------------+------------+-----------+----------+-------------------+-------------------+-----------------+-------------+\n",
            "+----------------------------+-------+---------------------+--------------------+------------------------------------+-----------+---------+--------+-----------------+-----------------+---------------+-----------+\n",
            "| Metrics on validation data |  Net  |    loss_function    | trained_till_epoch | lowest val_loss which was on epoch |  val_loss | val_acc | val_f1 | val_specificity | val_sensitivity | val_precision | val_auroc |\n",
            "+----------------------------+-------+---------------------+--------------------+------------------------------------+-----------+---------+--------+-----------------+-----------------+---------------+-----------+\n",
            "|           report           | UNet1 | binary_crossentropy |         40         |                 14                 |   0.2181  |  0.8011 | 0.4935 |      0.806      |      0.806      |     0.8054    |   0.806   |\n",
            "|           report           | UNet1 |      dice_loss      |        Nan         |                Nan                 |   0.1992  |  0.8008 | 0.4933 |      0.8016     |      0.8016     |     0.8011    |   0.8016  |\n",
            "|           report           | UNet1 |      focal_loss     |        Nan         |                Nan                 | 4232.2256 |  0.7367 | 0.4396 |      0.704      |      0.8016     |     0.7028    |   0.704   |\n",
            "|           report           | UNet2 | binary_crossentropy |         71         |                 33                 |   0.2361  |  0.7802 | 0.4762 |      0.8205     |      0.8205     |     0.8201    |   0.8205  |\n",
            "+----------------------------+-------+---------------------+--------------------+------------------------------------+-----------+---------+--------+-----------------+-----------------+---------------+-----------+\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}