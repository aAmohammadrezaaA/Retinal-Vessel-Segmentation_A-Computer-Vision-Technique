{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aAmohammadrezaaA/Retinal-Vessel-Segmentation_A-Computer-Vision-Technique/blob/main/implementation_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d4E8HHv8sLCr"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabulate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8128\\1710360181.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtabulate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mprettytable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAveragePooling2D\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mConv2DTranspose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mAdd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPool2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mUpSampling2D\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'"
     ]
    }
   ],
   "source": [
    "import tabulate\n",
    "from tabulate import tabulate\n",
    "from prettytable import PrettyTable\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import AveragePooling2D,Conv2DTranspose,Input,Add,Conv2D, BatchNormalization,LeakyReLU, Activation, MaxPool2D, Dropout, Flatten, Dense,UpSampling2D,Concatenate,Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "FAZqlwTN1z9I"
   },
   "outputs": [],
   "source": [
    "patch_size=48\n",
    "patch_num=1500\n",
    "patch_threshold=25\n",
    "BATCH_SIZE=64\n",
    "LR=0.0003\n",
    "channels=3\n",
    "\n",
    "input_shape = (patch_size, patch_size, channels)  # Adjusting the shape\n",
    "batch_size = BATCH_SIZE\n",
    "\n",
    "# random input tensor for testing\n",
    "input_tensor = tf.random.normal((batch_size,) + input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "gLm3ZDh-52ue"
   },
   "outputs": [],
   "source": [
    "class LinearTransform(tf.keras.Model):\n",
    "  def __init__(self, name=\"LinearTransform\"):\n",
    "    super(LinearTransform, self).__init__(self,name=name)\n",
    "\n",
    "    self.conv_r=Conv2D(1,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    self.conv_g=Conv2D(1,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    self.conv_b=Conv2D(1,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.pool_rc=AveragePooling2D(pool_size=(patch_size,patch_size),strides=1)\n",
    "    self.pool_gc=AveragePooling2D(pool_size=(patch_size,patch_size),strides=1)\n",
    "    self.pool_bc=AveragePooling2D(pool_size=(patch_size,patch_size),strides=1)\n",
    "\n",
    "    self.bn=BatchNormalization()\n",
    "    self.sigmoid=Activation('sigmoid')\n",
    "    self.softmax=Activation('softmax')\n",
    "\n",
    "  def call(self, input,training=True):\n",
    "    r,g,b=input[:,:,:,0:1],input[:,:,:,1:2],input[:,:,:,2:3]\n",
    "\n",
    "    rs=self.conv_r(r)\n",
    "    gs=self.conv_g(g)\n",
    "    bs=self.conv_r(b)\n",
    "\n",
    "    rc=tf.reshape(self.pool_rc(rs),[-1,1])\n",
    "    gc=tf.reshape(self.pool_gc(gs),[-1,1])\n",
    "    bc=tf.reshape(self.pool_bc(bs),[-1,1])\n",
    "\n",
    "    merge=Concatenate(axis=-1)([rc,gc,bc])\n",
    "    merge=tf.expand_dims(merge,axis=1)\n",
    "    merge=tf.expand_dims(merge,axis=1)\n",
    "    merge=self.softmax(merge)\n",
    "    merge=tf.repeat(merge,repeats=48,axis=2)\n",
    "    merge=tf.repeat(merge,repeats=48,axis=1)\n",
    "\n",
    "    r=r*(1+self.sigmoid(rs))\n",
    "    g=g*(1+self.sigmoid(gs))\n",
    "    b=b*(1+self.sigmoid(bs))\n",
    "\n",
    "    output=self.bn(merge[:,:,:,0:1]*r+merge[:,:,:,1:2]*g+merge[:,:,:,2:3]*b,training=training)\n",
    "    return output\n",
    "\n",
    "class ResBlock(tf.keras.Model):\n",
    "  def __init__(self,out_ch,residual_path=False,stride=1):\n",
    "    super(ResBlock,self).__init__(self)\n",
    "    self.residual_path=residual_path\n",
    "\n",
    "    self.conv1=Conv2D(out_ch,kernel_size=3,strides=stride,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.bn1=BatchNormalization()\n",
    "    self.relu1=LeakyReLU()#Activation('leaky_relu')\n",
    "\n",
    "    self.conv2=Conv2D(out_ch,kernel_size=3,strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.bn2=BatchNormalization()\n",
    "\n",
    "    if residual_path:\n",
    "      self.conv_shortcut=Conv2D(out_ch,kernel_size=1,strides=stride,padding='same',use_bias=False)\n",
    "      self.bn_shortcut=BatchNormalization()\n",
    "\n",
    "    self.relu2=LeakyReLU()#Activation('leaky_relu')\n",
    "\n",
    "  def call(self,x,training=True):\n",
    "    xs=self.relu1(self.bn1(self.conv1(x),training=training))\n",
    "    xs=self.bn2(self.conv2(xs),training=training)\n",
    "\n",
    "    if self.residual_path:\n",
    "      x=self.bn_shortcut(self.conv_shortcut(x),training=training)\n",
    "    #print(x.shape,xs.shape)\n",
    "    xs=x+xs\n",
    "    return self.relu2(xs)\n",
    "\n",
    "class Unet(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Unet,self).__init__(self)\n",
    "    self.conv_init=LinearTransform()\n",
    "    self.resinit=ResBlock(16,residual_path=True)\n",
    "    self.up_sample=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.resup=ResBlock(32,residual_path=True)\n",
    "\n",
    "    self.pool1=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    self.resblock_down1=ResBlock(64,residual_path=True)\n",
    "    self.resblock_down11=ResBlock(64,residual_path=False)\n",
    "    self.pool2=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    self.resblock_down2=ResBlock(128,residual_path=True)\n",
    "    self.resblock_down21=ResBlock(128,residual_path=False)\n",
    "    self.pool3=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    self.resblock_down3=ResBlock(256,residual_path=True)\n",
    "    self.resblock_down31=ResBlock(256,residual_path=False)\n",
    "    self.pool4=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    self.resblock=ResBlock(512,residual_path=True)\n",
    "\n",
    "    self.unpool3=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.resblock_up3=ResBlock(256,residual_path=True)\n",
    "    self.resblock_up31=ResBlock(256,residual_path=False)\n",
    "\n",
    "    self.unpool2=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.resblock_up2=ResBlock(128,residual_path=True)\n",
    "    self.resblock_up21=ResBlock(128,residual_path=False)\n",
    "\n",
    "    self.unpool1=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.resblock_up1=ResBlock(64,residual_path=True)\n",
    "\n",
    "    self.unpool_final=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.resblock2=ResBlock(32,residual_path=True)\n",
    "\n",
    "    self.pool_final=MaxPool2D(pool_size=(2,2))\n",
    "    self.resfinal=ResBlock(32)\n",
    "\n",
    "    self.conv_final=Conv2D(1,kernel_size=1,strides=1,padding='same',use_bias=False)\n",
    "    self.bn_final=BatchNormalization()\n",
    "    self.act=Activation('sigmoid')\n",
    "\n",
    "  def call(self,x,training=True):\n",
    "    x_linear=self.conv_init(x,training=training)\n",
    "    x=self.resinit(x_linear,training=training)\n",
    "    x=self.up_sample(x)\n",
    "    x=self.resup(x,training=training)\n",
    "\n",
    "    stage1=self.pool1(x)\n",
    "    stage1=self.resblock_down1(stage1,training=training)\n",
    "    stage1=self.resblock_down11(stage1,training=training)\n",
    "\n",
    "    stage2=self.pool2(stage1)\n",
    "    stage2=self.resblock_down2(stage2,training=training)\n",
    "    stage2=self.resblock_down21(stage2,training=training)\n",
    "\n",
    "    stage3=self.pool3(stage2)\n",
    "    stage3=self.resblock_down3(stage3,training=training)\n",
    "    stage3=self.resblock_down31(stage3,training=training)\n",
    "\n",
    "    stage4=self.pool4(stage3)\n",
    "    stage4=self.resblock(stage4,training=training)\n",
    "\n",
    "    stage3=Concatenate(axis=3)([stage3,self.unpool3(stage4)])\n",
    "    stage3=self.resblock_up3(stage3,training=training)\n",
    "    stage3=self.resblock_up31(stage3,training=training)\n",
    "\n",
    "    stage2=Concatenate(axis=3)([stage2,self.unpool2(stage3)])\n",
    "    stage2=self.resblock_up2(stage2,training=training)\n",
    "    stage2=self.resblock_up21(stage2,training=training)\n",
    "\n",
    "    stage1=Concatenate(axis=3)([stage1,self.unpool1(stage2)])\n",
    "    stage1=self.resblock_up1(stage1,training=training)\n",
    "\n",
    "    x=Concatenate(axis=3)([x,self.unpool_final(stage1)])\n",
    "    x=self.resblock2(x,training=training)\n",
    "\n",
    "    x=self.pool_final(x)\n",
    "    x=self.resfinal(x,training=training)\n",
    "\n",
    "    seg_result=self.act(self.bn_final(self.conv_final(x),training=training))\n",
    "\n",
    "    return x_linear,seg_result\n",
    "class Unet2(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Unet2,self).__init__(self)\n",
    "    self.conv_init=LinearTransform()\n",
    "    #self.resinit=ResBlock(16,residual_path=True)\n",
    "    self.resinit=Conv2D(16,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    self.up_sample=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    #self.resup=ResBlock(32,residual_path=True)\n",
    "    self.resup=Conv2D(32,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.pool1=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    #self.resblock_down1=ResBlock(64,residual_path=True)\n",
    "    self.resblock_down1=Conv2D(64,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    #self.resblock_down11=ResBlock(64,residual_path=False)\n",
    "    self.resblock_down11=Conv2D(64,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    self.pool2=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    #self.resblock_down2=ResBlock(128,residual_path=True)\n",
    "    self.resblock_down2=Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    #self.resblock_down21=ResBlock(128,residual_path=False)\n",
    "    self.resblock_down21=Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    self.pool3=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    #self.resblock_down3=ResBlock(256,residual_path=True)\n",
    "    self.resblock_down3=Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    #self.resblock_down31=ResBlock(256,residual_path=False)\n",
    "    self.resblock_down31=Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    self.pool4=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    #self.resblock=ResBlock(512,residual_path=True)\n",
    "    self.resblock=Conv2D(512,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.unpool3=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    #self.resblock_up3=ResBlock(256,residual_path=True)\n",
    "    self.resblock_up3=Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    #self.resblock_up31=ResBlock(256,residual_path=False)\n",
    "    self.resblock_up31=Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.unpool2=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    #self.resblock_up2=ResBlock(128,residual_path=True)\n",
    "    self.resblock_up2=Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    #self.resblock_up21=ResBlock(128,residual_path=False)\n",
    "    self.resblock_up21=Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.unpool1=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    #self.resblock_up1=ResBlock(64,residual_path=True)\n",
    "    self.resblock_up1=Conv2D(64,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.unpool_final=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    #self.resblock2=ResBlock(32,residual_path=True)\n",
    "    self.resblock2=Conv2D(32,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.pool_final=MaxPool2D(pool_size=(2,2))\n",
    "    #self.resfinal=ResBlock(32)\n",
    "    self.resfinal=Conv2D(32,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.conv_final=Conv2D(1,kernel_size=1,strides=1,padding='same',use_bias=False)\n",
    "    self.bn_final=BatchNormalization()\n",
    "    self.act=Activation('sigmoid')\n",
    "\n",
    "  def call(self,x,training=True):\n",
    "    x_linear=self.conv_init(x,training=training)\n",
    "    x=self.resinit(x_linear,training=training)\n",
    "    x=self.up_sample(x)\n",
    "    x=self.resup(x,training=training)\n",
    "\n",
    "    stage1=self.pool1(x)\n",
    "    stage1=self.resblock_down1(stage1,training=training)\n",
    "    stage1=self.resblock_down11(stage1,training=training)\n",
    "\n",
    "    stage2=self.pool2(stage1)\n",
    "    stage2=self.resblock_down2(stage2,training=training)\n",
    "    stage2=self.resblock_down21(stage2,training=training)\n",
    "\n",
    "    stage3=self.pool3(stage2)\n",
    "    stage3=self.resblock_down3(stage3,training=training)\n",
    "    stage3=self.resblock_down31(stage3,training=training)\n",
    "\n",
    "    stage4=self.pool4(stage3)\n",
    "    stage4=self.resblock(stage4,training=training)\n",
    "\n",
    "    stage3=Concatenate(axis=3)([stage3,self.unpool3(stage4)])\n",
    "    stage3=self.resblock_up3(stage3,training=training)\n",
    "    stage3=self.resblock_up31(stage3,training=training)\n",
    "\n",
    "    stage2=Concatenate(axis=3)([stage2,self.unpool2(stage3)])\n",
    "    stage2=self.resblock_up2(stage2,training=training)\n",
    "    stage2=self.resblock_up21(stage2,training=training)\n",
    "\n",
    "    stage1=Concatenate(axis=3)([stage1,self.unpool1(stage2)])\n",
    "    stage1=self.resblock_up1(stage1,training=training)\n",
    "\n",
    "    x=Concatenate(axis=3)([x,self.unpool_final(stage1)])\n",
    "    x=self.resblock2(x,training=training)\n",
    "\n",
    "    x=self.pool_final(x)\n",
    "    x=self.resfinal(x,training=training)\n",
    "\n",
    "    seg_result=self.act(self.bn_final(self.conv_final(x),training=training))\n",
    "\n",
    "    return x_linear,seg_result\n",
    "\n",
    "class Unet3(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Unet3, self).__init__()\n",
    "\n",
    "        # Define the encoder layers\n",
    "        #self.encoder_conv_init=LinearTransform()\n",
    "        self.encoder_conv_init=Conv2D(1, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv1 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.encoder_pool1 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.encoder_conv2 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.encoder_pool2 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.encoder_conv3 = Conv2D(128, 3, activation='relu', padding='same')\n",
    "        self.encoder_pool3 = MaxPool2D(pool_size=(2, 2))\n",
    "\n",
    "        # Define the bottleneck layer\n",
    "        self.bottleneck_conv = Conv2D(512, 3, activation='relu', padding='same')\n",
    "\n",
    "        # Define the decoder layers\n",
    "        self.decoder_upsample1 = UpSampling2D(size=(2, 2))\n",
    "        self.decoder_conv4 = Conv2D(128, 3, activation='relu', padding='same')\n",
    "        self.decoder_upsample2 = UpSampling2D(size=(2, 2))\n",
    "        self.decoder_conv5 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.decoder_upsample3 = UpSampling2D(size=(2, 2))\n",
    "        self.decoder_conv6 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.decoder_output = Conv2D(1, 3, activation='sigmoid', padding='same')\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        # Encoder\n",
    "        x_linear=self.encoder_conv_init(x, training=training)\n",
    "        enc1 = self.encoder_conv1(x_linear, training=training)\n",
    "        enc1_pool = self.encoder_pool1(enc1)\n",
    "        enc2 = self.encoder_conv2(enc1_pool, training=training)\n",
    "        enc2_pool = self.encoder_pool2(enc2)\n",
    "        enc3 = self.encoder_conv3(enc2_pool, training=training)\n",
    "        enc3_pool = self.encoder_pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck_conv(enc3_pool, training=training)\n",
    "\n",
    "        # Decoder\n",
    "        dec1 = self.decoder_upsample1(bottleneck)\n",
    "        dec1 = Concatenate()([dec1, enc3])\n",
    "        dec1 = self.decoder_conv4(dec1, training=training)\n",
    "        dec2 = self.decoder_upsample2(dec1)\n",
    "        dec2 = Concatenate()([dec2, enc2])\n",
    "        dec2 = self.decoder_conv5(dec2, training=training)\n",
    "        dec3 = self.decoder_upsample3(dec2)\n",
    "        dec3 = Concatenate()([dec3, enc1])\n",
    "        dec3 = self.decoder_conv6(dec3, training=training)\n",
    "\n",
    "        # Output\n",
    "        seg_result = self.decoder_output(dec3, training=training)\n",
    "        return x_linear,seg_result\n",
    "\n",
    "class Unet4(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Unet4, self).__init__()\n",
    "\n",
    "        # Define the encoder layers\n",
    "        self.encoder_conv_init=LinearTransform()\n",
    "        #self.encoder_conv_init=Conv2D(1, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv1 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.encoder_pool1 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.encoder_conv2 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.encoder_pool2 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.encoder_conv3 = Conv2D(128, 3, activation='relu', padding='same')\n",
    "        self.encoder_pool3 = MaxPool2D(pool_size=(2, 2))\n",
    "\n",
    "        # Define the bottleneck layer\n",
    "        self.bottleneck_conv = Conv2D(512, 3, activation='relu', padding='same')\n",
    "\n",
    "        # Define the decoder layers\n",
    "        self.decoder_upsample1 = UpSampling2D(size=(2, 2))\n",
    "        self.decoder_conv4 = Conv2D(128, 3, activation='relu', padding='same')\n",
    "        self.decoder_upsample2 = UpSampling2D(size=(2, 2))\n",
    "        self.decoder_conv5 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.decoder_upsample3 = UpSampling2D(size=(2, 2))\n",
    "        self.decoder_conv6 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.decoder_output = Conv2D(1, 3, activation='sigmoid', padding='same')\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        # Encoder\n",
    "        x_linear=self.encoder_conv_init(x, training=training)\n",
    "        enc1 = self.encoder_conv1(x_linear, training=training)\n",
    "        enc1_pool = self.encoder_pool1(enc1)\n",
    "        enc2 = self.encoder_conv2(enc1_pool, training=training)\n",
    "        enc2_pool = self.encoder_pool2(enc2)\n",
    "        enc3 = self.encoder_conv3(enc2_pool, training=training)\n",
    "        enc3_pool = self.encoder_pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck_conv(enc3_pool, training=training)\n",
    "\n",
    "        # Decoder\n",
    "        dec1 = self.decoder_upsample1(bottleneck)\n",
    "        dec1 = Concatenate()([dec1, enc3])\n",
    "        dec1 = self.decoder_conv4(dec1, training=training)\n",
    "        dec2 = self.decoder_upsample2(dec1)\n",
    "        dec2 = Concatenate()([dec2, enc2])\n",
    "        dec2 = self.decoder_conv5(dec2, training=training)\n",
    "        dec3 = self.decoder_upsample3(dec2)\n",
    "        dec3 = Concatenate()([dec3, enc1])\n",
    "        dec3 = self.decoder_conv6(dec3, training=training)\n",
    "\n",
    "        # Output\n",
    "        seg_result = self.decoder_output(dec3, training=training)\n",
    "        return x_linear,seg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oj-hkaz0sK_t",
    "outputId": "ab10a1ff-eea2-40e1-c907-f87b5cef43e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════╤═════════════════╤═══════════════════╤═══════════════════════════════════════════════════════════════════════════════╤══════════════╤═════════════════╕\n",
      "│ #       │ patch_size      │ patch_num         │ patch_threshold                                                               │ batch_size   │ learning_rate   │\n",
      "╞═════════╪═════════════════╪═══════════════════╪═══════════════════════════════════════════════════════════════════════════════╪══════════════╪═════════════════╡\n",
      "│ value   │ 48              │ 1500              │ 25                                                                            │ 64           │ 0.0003          │\n",
      "├─────────┼─────────────────┼───────────────────┼───────────────────────────────────────────────────────────────────────────────┼──────────────┼─────────────────┤\n",
      "│ comment │ (48*48) windows │ number of windows │ threshold for the patch, the smaller threshoold, the less vessel in the patch │ batch        │ LR              │\n",
      "╘═════════╧═════════════════╧═══════════════════╧═══════════════════════════════════════════════════════════════════════════════╧══════════════╧═════════════════╛\n"
     ]
    }
   ],
   "source": [
    "cols = [\"#\", \"patch_size\", \"patch_num\", \"patch_threshold\", \"batch_size\", \"learning_rate\"]\n",
    "rows = [[ \"value\", patch_size, patch_num, patch_threshold, BATCH_SIZE, LR],\n",
    "        [\"comment\", \"(48*48) windows\", \"number of windows\", \"threshold for the patch, the smaller threshoold, the less vessel in the patch\", \"batch\", \"LR\"]]\n",
    "\n",
    "print(tabulate(rows, headers=cols,tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "Ne9nQmN056ou"
   },
   "outputs": [],
   "source": [
    "model_unet1=Unet()\n",
    "model_unet2=Unet2()\n",
    "model_unet3=Unet3()\n",
    "model_unet4=Unet4()\n",
    "linear_output1, seg_result1 = model_unet1(input_tensor)\n",
    "linear_output2, seg_result2 = model_unet2(input_tensor)\n",
    "linear_output3, seg_result3 = model_unet3(input_tensor)\n",
    "linear_output4, seg_result4 = model_unet4(input_tensor)\n",
    "#model.summary(line_length=110)\n",
    "#model_unet1.count_params()\n",
    "#model_unet2.count_params()\n",
    "#model_unet3.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOUS51M47Wtv",
    "outputId": "f1065238-617a-4b73-c3bb-3be7b80d7f4e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PrettyTable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8128\\3213611948.py\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtrained_till_epoch_u2_bc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m71\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mlowest_val_loss_on_epoch_u2_bc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m33\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfield_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34mf\"Config of Net\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Net\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Number of parameters\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"comments\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"report\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"UNet1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_unet1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"The Unet architecture which uses residual blocks and residual path and skip connections\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PrettyTable' is not defined"
     ]
    }
   ],
   "source": [
    "train_loss_bc=0.0417;train_acc_bc=0.9228;train_f1_bc=0.5987;train_sp_bc=0.8952;train_se_bc=0.8952;train_precision_bc=0.894;train_auroc_bc=0.8952\n",
    "val_loss_bc=0.2181;val_acc_bc=0.8011;val_f1_bc=0.4935;val_sp_bc=0.8060;val_se_bc=0.8060;val_precision_bc=0.8054;val_auroc_bc=0.8060\n",
    "trained_till_epoch_bc=40; lowest_val_loss_on_epoch_bc=14\n",
    "\n",
    "train_loss_d=0.0782; train_acc_d=0.9218; train_f1_d=0.5979; train_sp_d=0.8926; train_se_d=0.8926; train_precision_d=0.8920; train_auroc_d=0.8926\n",
    "val_loss_d=0.1992; val_acc_d=0.8008; val_f1_d=0.4933; val_sp_d=0.8016; val_se_d=0.8016; val_precision_d=0.8011; val_auroc_d=0.8016\n",
    "trained_till_epoch_d=\"Nan\"; lowest_val_loss_on_epoch_d=\"Nan\"\n",
    "\n",
    "train_loss_f=170.6789; train_acc_f=0.8527; train_f1_f=0.5390; train_sp_f=0.7866; train_se_f=0.7040; train_precision_f=0.7859; train_auroc_f=0.7866\n",
    "val_loss_f=4232.2256; val_acc_f=0.7367; val_f1_f=0.4396; val_sp_f=0.7040; val_se_f=0.8016; val_precision_f=0.7028; val_auroc_f=0.7040\n",
    "trained_till_epoch_f=\"Nan\"; lowest_val_loss_on_epoch_f=\"Nan\"\n",
    "\n",
    "train_loss_u2_bc=0.0693; train_acc_u2_bc=0.8881; train_f1_u2_bc=0.5682; train_sp_u2_bc=0.8840; train_se_u2_bc=0.8840; train_precision_u2_bc=0.8833; train_auroc_u2_bc=0.8840\n",
    "val_loss_u2_bc=0.2361; val_acc_u2_bc=0.7802; val_f1_u2_bc=0.4762; val_sp_u2_bc=0.8205; val_se_u2_bc=0.8205; val_precision_u2_bc=0.8201; val_auroc_u2_bc=0.8205\n",
    "trained_till_epoch_u2_bc=71; lowest_val_loss_on_epoch_u2_bc=33;\n",
    "\n",
    "train_loss_u3_bc=0.09351323; train_acc_u3_bc=0.8605181; train_f1_u3_bc=0.5424623; train_sp_u3_bc=0.86103207; train_se_u3_bc=0.86103207; train_precision_u3_bc=0.860599; train_auroc_u3_bc=0.86102957\n",
    "val_loss_u3_bc=0.2161307; val_acc_u3_bc=0.7964834; val_f1_u3_bc=0.48970032; val_sp_u3_bc=0.81221986; val_se_u3_bc=0.81221986; val_precision_u3_bc=0.81199735; val_auroc_u3_bc=0.812216\n",
    "trained_till_epoch_u3_bc=33; lowest_val_loss_on_epoch_u3_bc=9;\n",
    "\n",
    "train_loss_u4_bc=0.0733392; train_acc_u4_bc=0.88768107; train_f1_u4_bc=0.567843; train_sp_u4_bc=0.88820463; train_se_u4_bc=0.88820463; train_precision_u4_bc=0.88785917; train_auroc_u4_bc=0.8882016\n",
    "val_loss_u4_bc=0.14747; val_acc_u4_bc=0.83952; val_f1_u4_bc=0.52617; val_sp_u4_bc=0.86361; val_se_u4_bc=0.86361; val_precision_u4_bc=0.86396; val_auroc_u4_bc=0.86361\n",
    "trained_till_epoch_u4_bc=35; lowest_val_loss_on_epoch_u4_bc=6; highest_val_acc_on_epoch_u4_bc=30; highest_val_f1_on_epoch_u4_bc=30; highest_val_sp_on_epoch_u4_bc=27; highest_val_se_on_epoch_u4_bc=27;highest_val_prec_on_epoch_u4_bc=27;highest_val_auroc_on_epoch_u4_bc=27\n",
    "\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = [f\"Net Config\", \"Net\", \"Number of parameters\", \"comments\"]\n",
    "x.add_row([\"report\", \"UNet1\", model_unet1.count_params(), \"The Unet architecture which uses residual blocks and residual path and skip connections\"])\n",
    "x.add_row([\"report\", \"UNet2\", model_unet2.count_params(), \"The same UNet, but replacing all the res blocks with Conv2D\"])\n",
    "x.add_row([\"report\", \"UNet3\", model_unet3.count_params(), \"This is a basic UNet model without even the linear transform for preprocessing\"])\n",
    "x.add_row([\"report\", \"UNet4\", model_unet4.count_params(), \"The same basic UNet with linear transform for preprocessing\"])\n",
    "print(x)\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = [f\"Metrics train data\", \"Net\", \"loss_function\", \"train_loss\", \"train_acc\", \"train_f1\", \"train_specificity\", \"train_sensitivity\", \"train_precision\", \"train_auroc\"]\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_bc}\", \"UNet1\", \"binary_crossentropy\", train_loss_bc, train_acc_bc, train_f1_bc, train_sp_bc, train_se_bc, train_precision_bc, train_auroc_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_d}\", \"UNet1\", \"dice_loss\", train_loss_d, train_acc_d, train_f1_d, train_sp_d, train_se_d, train_precision_d, train_auroc_d])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_f}\", \"UNet1\", \"focal_loss\", train_loss_f, train_acc_f, train_f1_f, train_sp_f, train_se_f, train_precision_f, train_auroc_f])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_u2_bc}\", \"UNet2\", \"binary_crossentropy\", train_loss_u2_bc, train_acc_u2_bc, train_f1_u2_bc, train_sp_u2_bc, train_se_u2_bc, train_precision_u2_bc, train_auroc_u2_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_u3_bc}\", \"UNet3\", \"binary_crossentropy\", train_loss_u3_bc, train_acc_u3_bc, train_f1_u3_bc, train_sp_u3_bc, train_se_u3_bc, train_precision_u3_bc, train_auroc_u3_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_u4_bc}\", \"UNet4\", \"binary_crossentropy\", train_loss_u4_bc, train_acc_u4_bc, train_f1_u4_bc, train_sp_u4_bc, train_se_u4_bc, train_precision_u4_bc, train_auroc_u4_bc])\n",
    "print(x)\n",
    "\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = [f\"Metrics val data\", \"Net\", \"loss_function\", \"val_loss\", \"val_acc\", \"val_f1\", \"val_specificity\", \"val_sensitivity\", \"val_precision\", \"val_auroc\"]\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_bc}\", \"UNet1\", \"binary_crossentropy\", val_loss_bc, val_acc_bc, val_f1_bc, val_sp_bc, val_se_bc, val_precision_bc, val_auroc_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_d}\", \"UNet1\", \"dice_loss\", val_loss_d, val_acc_d, val_f1_d, val_sp_d, val_se_d, val_precision_d, val_auroc_d])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_f}\", \"UNet1\", \"focal_loss\", val_loss_f, val_acc_f, val_f1_f, val_sp_f, val_se_f, val_precision_f, val_auroc_f])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_u2_bc}\", \"UNet2\", \"binary_crossentropy\", val_loss_u2_bc, val_acc_u2_bc, val_f1_u2_bc, val_sp_u2_bc, val_se_u2_bc, val_precision_u2_bc, val_auroc_u2_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_u3_bc}\", \"UNet3\", \"binary_crossentropy\", val_loss_u3_bc, val_acc_u3_bc, val_f1_u3_bc, val_sp_u3_bc, val_se_u3_bc, val_precision_u3_bc, val_auroc_u3_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_u4_bc}\", \"UNet4\", \"binary_crossentropy\", val_loss_u4_bc, val_acc_u4_bc, val_f1_u4_bc, val_sp_u4_bc, val_se_u4_bc, val_precision_u4_bc, val_auroc_u4_bc])\n",
    "print(x)\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = [f\"val metrics changes on epochs\", \"trained epoch\", \"lowest val_loss epoch\", \"highest val_acc epoch\", \"highest val_f1 epoch\", \"highest val_sp epoch\", \"highest val_se epoch\", \"highest val_prec epoch\", \"highest val_auroc epoch\"]\n",
    "x.add_row([\"UNet1_binarycrossentropy\", trained_till_epoch_bc, lowest_val_loss_on_epoch_bc, \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\"])\n",
    "x.add_row([\"UNet1_dice_loss\", trained_till_epoch_d, lowest_val_loss_on_epoch_d, \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\"])\n",
    "x.add_row([\"UNet1_focal_loss\", trained_till_epoch_f, lowest_val_loss_on_epoch_f, \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\"])\n",
    "x.add_row([\"UNet2_binarycrossentropy\", trained_till_epoch_u2_bc, lowest_val_loss_on_epoch_u2_bc, \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\"])\n",
    "x.add_row([\"UNet3_binarycrossentropy\", trained_till_epoch_u3_bc, lowest_val_loss_on_epoch_u3_bc, \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\"])\n",
    "x.add_row([\"UNet4_binarycrossentropy\", trained_till_epoch_u4_bc, lowest_val_loss_on_epoch_u4_bc, \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\"])\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Welcome To Colaboratory",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
